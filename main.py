# main.py
import pandas as pd
import numpy as np
from src.data_processing import load_and_preprocess_data
from src.linear_regression import add_bias_column, normal_equation, predict
from sklearn.linear_model import LinearRegression
from src.evaluate import (
    mean_squared_error, 
    root_mean_squared_error, 
    r_squared,
    mean_absolute_error, 
    mean_absolute_scaled_error 
)
from sklearn.model_selection import KFold, train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import  XGBRegressor
import matplotlib.pyplot as plt


def tune_lambda_with_kfold(X, y_transformed, y_orig, lambda_values, inverse_fn, n_splits=5):
    """
    Ch·ªçn lambda t·ªët nh·∫•t b·∫±ng K-Fold CV tr√™n d·ªØ li·ªáu train, ƒë√°nh gi√° tr√™n thang ƒëo g·ªëc.
    """
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    best_lambda = None
    best_rmse = float("inf")

    for lam in lambda_values:
        fold_rmses = []
        print(f"ƒêang th·ª≠ nghi·ªám v·ªõi lambda = {lam}")
        for train_idx, val_idx in kf.split(X):
            X_tr, X_val = X[train_idx], X[val_idx]
            y_tr_transformed = y_transformed[train_idx]
            y_val_orig = y_orig[val_idx]

            theta = normal_equation(X_tr, y_tr_transformed, lambda_reg=lam)
            if theta is None:
                continue

            y_val_pred_transformed = predict(X_val, theta)
            y_val_pred = inverse_fn(y_val_pred_transformed)
            rmse = root_mean_squared_error(y_val_orig, y_val_pred)
            fold_rmses.append(rmse)

        if not fold_rmses:
            print("  Kh√¥ng th·ªÉ t√≠nh RMSE cho lambda n√†y, b·ªè qua.")
            continue

        avg_rmse = float(np.mean(fold_rmses))
        print(f"  -> RMSE trung b√¨nh (CV): {avg_rmse:.4f}")

        if avg_rmse < best_rmse:
            best_rmse = avg_rmse
            best_lambda = lam
            print("  *** T√¨m th·∫•y lambda t·ªët h∆°n th√¥ng qua CV! ***")

    return best_lambda, best_rmse


def inverse_log_transform(y_log_values, clip_min=-20, clip_max=20):
    """
    Chuy·ªÉn c√°c gi√° tr·ªã log1p v·ªÅ thang ƒëo g·ªëc v·ªõi clipping ƒë·ªÉ tr√°nh overflow.
    """
    clipped = np.clip(y_log_values, clip_min, clip_max)
    return np.expm1(clipped)


def identity_transform(values, *_args, **_kwargs):
    """H√†m gi·ªØ nguy√™n gi√° tr·ªã (d√πng cho bi·∫øn th·ªÉ target g·ªëc)."""
    return values

def run_training_pipeline():
    
    DATA_PATH = "data/vietnam_housing_dataset.csv" # S·ª≠a l·∫°i t√™n file n·∫øu c·∫ßn
    print("--- B∆Ø·ªöC 1: T·∫¢I V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU ---")
    X, y, feature_names = load_and_preprocess_data(DATA_PATH)
    
    if X is None or y is None:
        print("K·∫øt th√∫c do l·ªói t·∫£i d·ªØ li·ªáu.")
        return

    print("\n--- B∆Ø·ªöC 2: TH√äM C·ªòT BIAS ---")
    X_b = add_bias_column(X)
    print(f"T·ªïng s·ªë m·∫´u: {X_b.shape[0]}, T·ªïng s·ªë ƒë·∫∑c tr∆∞ng (c√≥ bias): {X_b.shape[1]}")

    print("\n--- B∆Ø·ªöC 3: CHIA D·ªÆ LI·ªÜU (TRAIN/TEST) ---")
    y_log = np.log1p(np.maximum(y, 0))
    (
        X_train,
        X_test,
        y_train_log,
        _y_test_log,
        y_train_orig,
        y_test_orig,
    ) = train_test_split(X_b, y_log, y, test_size=0.2, random_state=42)
    print(f"K√≠ch th∆∞·ªõc t·∫≠p Train: {X_train.shape[0]} m·∫´u")
    print(f"K√≠ch th∆∞·ªõc t·∫≠p Test: {X_test.shape[0]} m·∫´u")

    # --- B∆Ø·ªöC 4: TINH CH·ªàNH SI√äU THAM S·ªê (lambda) ---
    print("\n--- B∆Ø·ªöC 4: TINH CH·ªàNH SI√äU THAM S·ªê (lambda) ---")
    
    lambda_values_to_try = [0.001, 0.01, 0.1, 1, 10, 100]
    target_variants = [
        {
            "name": "log1p(target)",
            "y_train": y_train_log,
            "inverse_fn": inverse_log_transform,
        },
        {
            "name": "target g·ªëc",
            "y_train": y_train_orig,
            "inverse_fn": identity_transform,
        },
    ]

    best_variant = None
    for variant in target_variants:
        print(f"\n>>> ƒê√°nh gi√° bi·∫øn th·ªÉ target: {variant['name']}")
        lam, cv_rmse = tune_lambda_with_kfold(
            X_train,
            variant["y_train"],
            y_train_orig,
            lambda_values_to_try,
            variant["inverse_fn"],
        )
        if lam is None:
            continue

        if (best_variant is None) or (cv_rmse < best_variant["cv_rmse"]):
            best_variant = {
                "name": variant["name"],
                "lambda": lam,
                "cv_rmse": cv_rmse,
                "y_train": variant["y_train"],
                "inverse_fn": variant["inverse_fn"],
            }

    if best_variant is None:
        print("Kh√¥ng t√¨m ƒë∆∞·ª£c c·∫•u h√¨nh target n√†o ph√π h·ª£p, k·∫øt th√∫c pipeline.")
        return

    best_lambda = best_variant["lambda"]
    inverse_target_fn = best_variant["inverse_fn"]
    y_train_for_fit = best_variant["y_train"]

    print("\n--- K·∫æT TH√öC TINH CH·ªàNH ---")
    print(
        f"==> C·∫•u h√¨nh: {best_variant['name']} v·ªõi lambda = {best_lambda} "
        f"(RMSE trung b√¨nh CV = {best_variant['cv_rmse']:.4f})"
    )

    theta = normal_equation(X_train, y_train_for_fit, lambda_reg=best_lambda)
    
    if theta is None:
        print("K·∫øt th√∫c do l·ªói hu·∫•n luy·ªán (kh√¥ng t√¨m th·∫•y theta n√†o h·ª£p l·ªá).")
        return
        
    print("Hu·∫•n luy·ªán th√†nh c√¥ng v·ªõi lambda t·ªët nh·∫•t.")


    print("\n--- B∆Ø·ªöC 5: ƒê√ÅNH GI√Å M√î H√åNH T·ª∞ T·∫†O T·ªêT NH·∫§T (TR√äN T·∫¨P TEST) ---")
    
    # T·∫°o d·ª± ƒëo√°n cu·ªëi c√πng b·∫±ng m√¥ h√¨nh t·ªët nh·∫•t
    y_pred_transformed = predict(X_test, theta)
    y_pred = inverse_target_fn(y_pred_transformed)
    
    # T√≠nh to√°n t·∫•t c·∫£ c√°c ch·ªâ s·ªë
    mse = mean_squared_error(y_test_orig, y_pred)
    rmse = root_mean_squared_error(y_test_orig, y_pred)
    r2 = r_squared(y_test_orig, y_pred)    
    mae = mean_absolute_error(y_test_orig, y_pred)
    mase = mean_absolute_scaled_error(y_test_orig, y_pred, y_train_orig) 
    
    print("\n--- K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH CU·ªêI C√ôNG ---")
    print(f" (Bi·∫øn th·ªÉ target: {best_variant['name']} | lambda = {best_lambda})")
    print(f"  R-squared (H·ªá s·ªë X√°c ƒë·ªãnh):     {r2:.4f} (ho·∫∑c {r2*100:.2f}%)")
    print(f"  MSE (L·ªói B√¨nh ph∆∞∆°ng Trung b√¨nh): {mse:.4f}")
    print(f"  RMSE (L·ªói Trung b√¨nh):           {rmse:.4f}")
    print(f"  MAE (L·ªói Tuy·ªát ƒë·ªëi Trung b√¨nh):  {mae:.4f}")  
    print(f"  MASE (L·ªói Co gi√£n Trung b√¨nh):   {mase:.4f}") 

    # V·∫º S∆† ƒê·ªí
    plt.figure(figsize=(8, 6))

# V·∫Ω scatter ƒëi·ªÉm d·ª± ƒëo√°n
    plt.scatter(y_test_orig, y_pred, alpha=0.7)

    # V·∫Ω ƒë∆∞·ªùng tham chi·∫øu y = x
    min_val = min(min(y_test_orig), min(y_pred))
    max_val = max(max(y_test_orig), max(y_pred))

    plt.plot([min_val, max_val], [min_val, max_val], linewidth=2)

    plt.xlabel("Gi√° tr·ªã th·∫≠t (y_test)")
    plt.ylabel("Gi√° tr·ªã d·ª± ƒëo√°n (y_pred)")
    plt.title("So s√°nh Gi√° tr·ªã Th·∫≠t vs D·ª± ƒëo√°n (Prediction Plot)")
    plt.grid(True)

    plt.show()
 

    # So s√°nh v·ªõi Linear Regression t·ª´ sklearn
    lr = LinearRegression()
    lr.fit(X_train, y_train_for_fit)   # üëâ d√πng ƒë√∫ng target ƒë√£ d√πng cho m√¥ h√¨nh t·ª± t·∫°o

    lr_pred_transformed = lr.predict(X_test)
    lr_pred = inverse_target_fn(lr_pred_transformed)

    # -----------------------------------------------------
    # üîç T√çNH TO√ÅN CH·ªà S·ªê CHO M√î H√åNH LINEAR REGRESSION (sklearn)
    # -----------------------------------------------------
    lr_mse = mean_squared_error(y_test_orig, lr_pred)
    lr_rmse = root_mean_squared_error(y_test_orig, lr_pred)
    lr_r2 = r_squared(y_test_orig, lr_pred)
    lr_mae = mean_absolute_error(y_test_orig, lr_pred)
    lr_mase = mean_absolute_scaled_error(y_test_orig, lr_pred, y_train_orig)

    print("\n--- K·∫æT QU·∫¢ M√î H√åNH LINEAR REGRESSION (sklearn) ---")
    print(f"  R-squared:                      {lr_r2:.4f}")
    print(f"  MSE:                            {lr_mse:.4f}")
    print(f"  RMSE:                           {lr_rmse:.4f}")
    print(f"  MAE:                            {lr_mae:.4f}")
    print(f"  MASE:                           {lr_mase:.4f}")

    # =====================================================
    # üéØ ƒê·ªò TR√ôNG KH·ªöP GI·ªÆA M√î H√åNH T·ª∞ T·∫†O V√Ä SKLEARN
    # =====================================================
    diff = np.abs(y_pred - lr_pred)
    avg_diff = diff.mean()
    max_diff = diff.max()
    min_diff = diff.min()

    print("\n=========== ƒê·ªò TR√ôNG KH·ªöP GI·ªÆA HAI M√î H√åNH ===========")
    print(f"Ch√™nh l·ªách trung b√¨nh (|y_self - y_sklearn|):  {avg_diff:.4f}")
    print(f"Ch√™nh l·ªách l·ªõn nh·∫•t:                           {max_diff:.4f}")
    print(f"Ch√™nh l·ªách nh·ªè nh·∫•t:                           {min_diff:.4f}")


    # =====================================================
    # üéØ XGBOOST REGRESSOR ‚Äî SO S√ÅNH M·∫†NH NH·∫§T
    # =====================================================
    xgb = XGBRegressor(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=8,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="reg:squarederror",
        tree_method="hist",
        random_state=42,
        n_jobs=-1
    )


    # Hu·∫•n luy·ªán XGBoost (d√πng target g·ªëc)
    xgb.fit(X_train, y_train_orig)

    # D·ª± ƒëo√°n
    xgb_pred = xgb.predict(X_test)

    # T√≠nh to√°n c√°c ch·ªâ s·ªë
    xgb_mse = mean_squared_error(y_test_orig, xgb_pred)
    xgb_rmse = root_mean_squared_error(y_test_orig, xgb_pred)
    xgb_r2 = r_squared(y_test_orig, xgb_pred)
    xgb_mae = mean_absolute_error(y_test_orig, xgb_pred)
    xgb_mase = mean_absolute_scaled_error(y_test_orig, xgb_pred, y_train_orig)

    print("\n--- K·∫æT QU·∫¢ M√î H√åNH XGBOOST REGRESSOR ---")
    print(f"  R-squared: {xgb_r2:.4f}")
    print(f"  MSE:       {xgb_mse:.4f}")
    print(f"  RMSE:      {xgb_rmse:.4f}")
    print(f"  MAE:       {xgb_mae:.4f}")
    print(f"  MASE:      {xgb_mase:.4f}")

if __name__ == "__main__":
    run_training_pipeline()
